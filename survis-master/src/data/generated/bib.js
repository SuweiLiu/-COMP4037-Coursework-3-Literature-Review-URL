const generatedBibEntries = {
    "8460962": {
        "author": "Milioto, Andres and Lottes, Philipp and Stachniss, Cyrill",
        "booktitle": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
        "doi": "10.1109/ICRA.2018.8460962",
        "number": "",
        "pages": "2229-2235",
        "title": "Real-Time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2018"
    },
    "8826846": {
        "author": "Hussmann, Stephan and Wang, Yuheng and Czymmek, Vitali and Knoll, Florian J.",
        "booktitle": "2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)",
        "doi": "10.1109/I2MTC.2019.8826846",
        "number": "",
        "pages": "1-6",
        "title": "Image matching algorithm for weed control applications in organic farming",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2019"
    },
    "9001903": {
        "author": "Fatma, Sabiha and Parimita Dash, Prajna",
        "booktitle": "2019 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",
        "doi": "10.1109/ICCECE44727.2019.9001903",
        "number": "",
        "pages": "1-5",
        "title": "Moment Invariant Based Weed/Crop Discrimination For Smart Farming",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2019"
    },
    "9298209": {
        "author": "Arun, R. Arumuga and Umamaheswari, S. and Jain, Ashvini Vimal",
        "booktitle": "2020 IEEE International Conference for Innovation in Technology (INOCON)",
        "doi": "10.1109/INOCON50539.2020.9298209",
        "number": "",
        "pages": "1-6",
        "title": "Reduced U-Net Architecture for Classifying Crop and Weed using Pixel-wise Segmentation",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2020"
    },
    "ABOUZAHIR2021179": {
        "abstract": "As season-long weeds competition produces important yield losses, early detection of these plants is essential to sustain productivity. Machine vision as a non-destructive surveying technique requires features that can describe weeds in a real field case. Colours and shapes provide good results in controlled conditions. However, when different crops or weeds appear in clusters, such solutions fail to meet satisfactory performance. Therefore, considering features that are less specific to field conditions is crucial for integrated weed management. In this study, we provide effective use of the Histogram of Oriented Gradients (HOG) to improve its performance for weed detection. The concept is based on the Bag-of-Visual-Words (BOVW) approach. We use the HOG blocks as keypoints to generate the visual-words, and the features vectors are the histograms of these visual-words. Next, we use the Backpropagation Neural Network to detect weeds and classify plants for three different crop fields. Namely, we consider sugar-beet, soybean, and carrot as target crops. Results demonstrate that the proposed weed detection system can locate weeds for site-specific treatment and selective spraying of herbicides. The proposed BOVW-based HOG can discriminate between weeds and crops with an accuracy of 97.7%, 93%, and 96.6% in sugar-beet, carrot and soybean fields respectively. For plant classification, our method can classify plants with an accuracy of 90.4%, 92.4%, and 94.1% in sugar-beet, carrot and soybean fields respectively. Our results turn out 37.6% better than the classical HOG that produces an accuracy ranging from 71.2% to 83.3% in weed detection and 49.1%\u201382.1% in plant classification.",
        "author": "Saad Abouzahir and Mohamed Sadik and Essaid Sabir",
        "doi": "https://doi.org/10.1016/j.biosystemseng.2020.11.005",
        "issn": "1537-5110",
        "journal": "Biosystems Engineering",
        "keywords": "Computer vision, Weed detection, Neural Network, Bag of visual words, Histogram of oriented gradients",
        "pages": "179-194",
        "title": "Bag-of-visual-words-augmented Histogram of Oriented Gradients for efficient weed detection",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S1537511020303056",
        "volume": "202",
        "year": "2021"
    },
    "AHMAD2021106081": {
        "abstract": "Knowing precise location and having accurate information about weed species is a prerequisite for developing an effective site-specific weed management (SSWM) system. Due to the effectiveness of deep learning techniques for vision-based tasks such as image classification and object detection, its use for discriminating between weeds and crops is gaining acceptance among the agricultural research community. However, limited studies have used deep learning for identifying multiple weeds in a single image and most of the studies have not compared the effectiveness of deep learning based image classification and object detection by using a common, annotated imagery dataset of early season weeds under field conditions. This study addresses the research gap by evaluating comparative performance of three different pre-trained image classification models for classifying weed species and also assesses the performance of an object detection model for locating and identifying weed species. The image classification models were trained on two commonly used deep learning frameworks i.e., Keras and PyTorch, to assess any performance differential due to the choice of framework. An annotated dataset comprising of RGB images of four, early season weeds, found in corn and soybean production system in the Midwest US, namely, cocklebur (Xanthium strumarium), foxtail (Setaria viridis), redroot pigweed (Amaranthus retroflexus), and giant ragweed (Ambrosia trifida) was used in this study. VGG16, ResNet50, and InceptionV3 pre-trained models were used for image classification. The object detection model, based on the You Only Look Once (YOLOv3) library, was trained to locate and identify different weed species within an image. The performance of image classification models was assessed using testing accuracy and F1-score metrics. Average precision (AP) and mean average precision (mAP) were used to assess the performance of the object detection model. The best performing image classification model was VGG16 with an accuracy of 98.90% and an F1-score of 99%. Faster training times and higher accuracies were observed with PyTorch. The detection model helped locate and identify multiple weeds within an image with AP scores of 43.28%, 26.30%, 89.89%, and 57.80% for cocklebur, foxtail, redroot pigweed, and giant ragweed respectively and an overall mAP score of 54.3%. The results suggest that under field conditions, use of pre-trained models for image classification and YOLOv3 for object detection are promising for identifying single and multiple weeds, respectively, given that sufficient data is available. Additionally, unlike image classification, the localization capabilities of object detection are desirable for developing a system for SSWM.",
        "author": "Aanis Ahmad and Dharmendra Saraswat and Varun Aggarwal and Aaron Etienne and Benjamin Hancock",
        "doi": "https://doi.org/10.1016/j.compag.2021.106081",
        "issn": "0168-1699",
        "journal": "Computers and Electronics in Agriculture",
        "keywords": "Site-Specific Weed Management, Weed identification, Image classification, Object detection",
        "pages": "106081",
        "title": "Performance of deep learning models for classifying and detecting common weeds in corn and soybean production systems",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0168169921000995",
        "volume": "184",
        "year": "2021"
    },
    "ASAD2020535": {
        "abstract": "Herbicide use is rising globally to enhance food production, causing harm to environment and the ecosystem. Precision agriculture suggests variable-rate herbicide application based on weed densities to mitigate adverse effects of herbicides. Accurate weed density estimation using advanced computer vision techniques like deep learning requires large labelled agriculture data. Labelling large agriculture data at pixel level is a time-consuming and tedious job. In this paper, a methodology is developed to accelerate manual labelling of pixels using a two-step procedure. In the first step, the background and foreground are segmented using maximum likelihood classification, and in the second step, the weed pixels are manually labelled. Such labelled data is used to train semantic segmentation models, which classify crop and background pixels as one class, and all other vegetation as the second class. This paper evaluates the proposed methodology on high-resolution colour images of canola fields and makes performance comparison of deep learning meta-architectures like SegNet and UNET and encoder blocks like VGG16 and ResNet-50. ResNet-50 based SegNet model has shown the best results with mean intersection over union value of 0.8288 and frequency weighted intersection over union value of 0.9869.",
        "author": "Muhammad Hamza Asad and Abdul Bais",
        "doi": "https://doi.org/10.1016/j.inpa.2019.12.002",
        "issn": "2214-3173",
        "journal": "Information Processing in Agriculture",
        "keywords": "Weed detection, Semantic segmentation, Variable rate herbicide, Maximum likelihood classification",
        "number": "4",
        "pages": "535-545",
        "title": "Weed detection in canola fields using maximum likelihood classification and deep convolutional neural network",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2214317319302355",
        "volume": "7",
        "year": "2020"
    },
    "ESPEJOGARCIA202179": {
        "abstract": "In recent years, automatic weed control has emerged as a promising alternative for reducing the amount of herbicide applied to the field, instead of conventional spraying. The use of artificial intelligence through the implementation of deep learning for early weeds identification has been one of the engines to boost this progress. However, these techniques usually need very large datasets coping with real-world conditions, which are scarce in the agricultural domain. To address the lack of such datasets, this paper proposes a methodology that combines the use of agricultural transfer learning and the creation of artificial images by generative adversarial networks (GANs). Several architectures and configurations have been evaluated on a dataset containing images of tomato and black nightshade. The best configuration was a combination of GANs creating plausible synthetic images and the Xception network, with a performance of 99.07% on the test set and 93.23% on a noisy version of the same set. Other architectures, such as Inception or DenseNet have also been evaluated, and they obtained promising results by using GANs. According to the results, the combination of advanced transfer learning and data augmentation techniques through GANs should be deeply studied in the future with more complex datasets.",
        "author": "Borja Espejo-Garcia and Nikos Mylonas and Loukas Athanasakos and Eleanna Vali and Spyros Fountas",
        "doi": "https://doi.org/10.1016/j.biosystemseng.2021.01.014",
        "issn": "1537-5110",
        "journal": "Biosystems Engineering",
        "keywords": "Image synthesis, Precision agriculture, Weeds identification, Deep learning, GAN, Transfer learning",
        "pages": "79-89",
        "title": "Combining generative adversarial networks and agricultural transfer learning for weeds identification",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S1537511021000155",
        "volume": "204",
        "year": "2021"
    },
    "ZHENG2017215": {
        "abstract": "An automated method for maize and weed detection is very important to efficiently remove weeds and precisely calculate the quantity of maize. Color features were used in this study to investigate a simple maize-detection method using a color machine-vision system. Conventional image segmentation methods based on RGB values cannot separate maize from weeds because of the highly similar image RGB values of these plants. Thus, a post-processing algorithm was developed to distinguish maize from weeds after image preprocessing. Color indices were used to develop a classification model. The nine optimal features were selected by principal component analysis to reduce the effect of illumination. Finally, support vector data description was used as a classifier to differentiate maize from the mixes of different species of weeds. Pictures were taken by a commercial camera and used to verify the stability of the algorithm. Results show that the overall accuracy for three years is 90.19%, 92.36% and 93.87%, respectively. And the color indices used in this work were stable under various weather conditions and over time.",
        "author": "Yang Zheng and Qibing Zhu and Min Huang and Ya Guo and Jianwei Qin",
        "doi": "https://doi.org/10.1016/j.compag.2017.07.028",
        "issn": "0168-1699",
        "journal": "Computers and Electronics in Agriculture",
        "keywords": "Maize detection, Color index, Support vector data description, Weed",
        "pages": "215-222",
        "title": "Maize and weed classification using color indices with support vector data description in outdoor fields",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0168169917301096",
        "volume": "141",
        "year": "2017"
    },
    "s20020455": {
        "abstract": "Precision weeding can significantly reduce or even eliminate the use of herbicides in farming. To achieve high-precision, individual targeting of weeds, high-speed, low-cost plant identification is essential. Our system using the red, green, and near-infrared reflectance, combined with a size differentiation method, is used to identify crops and weeds in lettuce fields. Illumination is provided by LED arrays at 525, 650, and 850 nm, and images are captured in a single-shot using a modified RGB camera. A kinematic stereo method is utilised to compensate for parallax error in images and provide accurate location data of plants. The system was verified in field trials across three lettuce fields at varying growth stages from 0.5 to 10 km/h. In-field results showed weed and crop identification rates of 56% and 69%, respectively. Post-trial processing resulted in average weed and crop identifications of 81% and 88%, respectively.",
        "article-number": "455",
        "author": "Elstone, Lydia and How, Kin Yau and Brodie, Samuel and Ghazali, Muhammad Zulfahmi and Heath, William P. and Grieve, Bruce",
        "doi": "10.3390/s20020455",
        "issn": "1424-8220",
        "journal": "Sensors",
        "number": "2",
        "pubmedid": "31947520",
        "title": "High Speed Crop and Weed Identification in Lettuce Fields for Precision Weeding",
        "type": "Article",
        "url": "https://www.mdpi.com/1424-8220/20/2/455",
        "volume": "20",
        "year": "2020"
    }
};